{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matty/BabyJoey/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/matty/BabyJoey/src/data/data.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  training_dataset = torch.load(self.train_file)\n",
      "/home/matty/BabyJoey/src/data/data.py:32: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  validation_dataset = torch.load(self.valid_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs shape: torch.Size([2, 512])\n",
      "Epoch 1/3\n",
      "Step 10, Loss: 10.8557\n",
      "Step 20, Loss: 10.4595\n",
      "Step 30, Loss: 9.8785\n",
      "Step 40, Loss: 9.0284\n",
      "Step 50, Loss: 8.7108\n",
      "Step 60, Loss: 7.8799\n",
      "Step 70, Loss: 7.8541\n",
      "Step 80, Loss: 8.2249\n",
      "Step 90, Loss: 8.7099\n",
      "Step 100, Loss: 9.1176\n",
      "Step 110, Loss: 8.3448\n",
      "Step 120, Loss: 8.0079\n",
      "Step 130, Loss: 9.0371\n",
      "Step 140, Loss: 8.2543\n",
      "Step 150, Loss: 8.4205\n",
      "Step 160, Loss: 8.5938\n",
      "Step 170, Loss: 7.4113\n",
      "Step 180, Loss: 7.4567\n",
      "Step 190, Loss: 9.2399\n",
      "Step 200, Loss: 7.2231\n",
      "Step 210, Loss: 7.1624\n",
      "Step 220, Loss: 9.2036\n",
      "Step 230, Loss: 7.9176\n",
      "Step 240, Loss: 7.5024\n",
      "Step 250, Loss: 8.0537\n",
      "Step 260, Loss: 5.3396\n",
      "Step 270, Loss: 7.8347\n",
      "Step 280, Loss: 7.8839\n",
      "Step 290, Loss: 7.3701\n",
      "Step 300, Loss: 8.0765\n",
      "Step 310, Loss: 7.1636\n",
      "Step 320, Loss: 7.9542\n",
      "Step 330, Loss: 7.9724\n",
      "Step 340, Loss: 8.3047\n",
      "Step 350, Loss: 7.5592\n",
      "Step 360, Loss: 8.4055\n",
      "Step 370, Loss: 7.3380\n",
      "Step 380, Loss: 7.5311\n",
      "Step 390, Loss: 8.4183\n",
      "Step 400, Loss: 8.5113\n",
      "Step 410, Loss: 7.5828\n",
      "Step 420, Loss: 7.8108\n",
      "Step 430, Loss: 7.3843\n",
      "Step 440, Loss: 8.0922\n",
      "Step 450, Loss: 8.8890\n",
      "Step 460, Loss: 8.3218\n",
      "Step 470, Loss: 8.7582\n",
      "Step 480, Loss: 7.8526\n",
      "Step 490, Loss: 8.7128\n",
      "Step 500, Loss: 6.5121\n",
      "Step 510, Loss: 7.7088\n",
      "Step 520, Loss: 7.1023\n",
      "Validation Loss: 7.6168\n",
      "Checkpoint saved at baby_joey_checkpoint_epoch_1.pt\n",
      "Epoch 2/3\n",
      "Step 10, Loss: 6.4086\n",
      "Step 20, Loss: 5.7634\n",
      "Step 30, Loss: 6.7787\n",
      "Step 40, Loss: 7.3052\n",
      "Step 50, Loss: 6.7423\n",
      "Step 60, Loss: 6.4615\n",
      "Step 70, Loss: 6.2066\n",
      "Step 80, Loss: 6.3136\n",
      "Step 90, Loss: 6.7231\n",
      "Step 100, Loss: 6.8495\n",
      "Step 110, Loss: 6.6074\n",
      "Step 120, Loss: 6.8134\n",
      "Step 130, Loss: 6.0702\n",
      "Step 140, Loss: 6.5212\n",
      "Step 150, Loss: 5.4103\n",
      "Step 160, Loss: 6.8915\n",
      "Step 170, Loss: 6.8416\n",
      "Step 180, Loss: 7.0020\n",
      "Step 190, Loss: 5.0552\n",
      "Step 200, Loss: 7.1156\n",
      "Step 210, Loss: 7.1358\n",
      "Step 220, Loss: 7.7211\n",
      "Step 230, Loss: 6.8766\n",
      "Step 240, Loss: 7.3905\n",
      "Step 250, Loss: 5.4056\n",
      "Step 260, Loss: 5.2070\n",
      "Step 270, Loss: 5.1308\n",
      "Step 280, Loss: 7.4469\n",
      "Step 290, Loss: 6.5554\n",
      "Step 300, Loss: 6.9295\n",
      "Step 310, Loss: 6.5337\n",
      "Step 320, Loss: 1.6518\n",
      "Step 330, Loss: 6.5304\n",
      "Step 340, Loss: 6.3176\n",
      "Step 350, Loss: 7.7329\n",
      "Step 360, Loss: 6.1255\n",
      "Step 370, Loss: 6.3359\n",
      "Step 380, Loss: 6.5572\n",
      "Step 390, Loss: 6.1698\n",
      "Step 400, Loss: 6.7222\n",
      "Step 410, Loss: 4.4826\n",
      "Step 420, Loss: 6.4168\n",
      "Step 430, Loss: 6.6395\n",
      "Step 440, Loss: 6.4522\n",
      "Step 450, Loss: 6.7008\n",
      "Step 460, Loss: 7.1368\n",
      "Step 470, Loss: 7.5036\n",
      "Step 480, Loss: 6.7620\n",
      "Step 490, Loss: 7.0447\n",
      "Step 500, Loss: 7.3182\n",
      "Step 510, Loss: 6.4243\n",
      "Step 520, Loss: 6.4151\n",
      "Validation Loss: 7.3910\n",
      "Checkpoint saved at baby_joey_checkpoint_epoch_2.pt\n",
      "Epoch 3/3\n",
      "Step 10, Loss: 5.7177\n",
      "Step 20, Loss: 5.6717\n",
      "Step 30, Loss: 6.8233\n",
      "Step 40, Loss: 5.5864\n",
      "Step 50, Loss: 3.7535\n",
      "Step 60, Loss: 4.6505\n",
      "Step 70, Loss: 5.5732\n",
      "Step 80, Loss: 5.4558\n",
      "Step 90, Loss: 6.0475\n",
      "Step 100, Loss: 5.7898\n",
      "Step 110, Loss: 6.0569\n",
      "Step 120, Loss: 2.0095\n",
      "Step 130, Loss: 5.2429\n",
      "Step 140, Loss: 5.9359\n",
      "Step 150, Loss: 5.3058\n",
      "Step 160, Loss: 6.0267\n",
      "Step 170, Loss: 4.3640\n",
      "Step 180, Loss: 1.7320\n",
      "Step 190, Loss: 5.4036\n",
      "Step 200, Loss: 4.6734\n",
      "Step 210, Loss: 6.0460\n",
      "Step 220, Loss: 6.0301\n",
      "Step 230, Loss: 5.9901\n",
      "Step 240, Loss: 5.3652\n",
      "Step 250, Loss: 5.6039\n",
      "Step 260, Loss: 5.7412\n",
      "Step 270, Loss: 5.7508\n",
      "Step 280, Loss: 5.9570\n",
      "Step 290, Loss: 6.2696\n",
      "Step 300, Loss: 5.7352\n",
      "Step 310, Loss: 6.0118\n",
      "Step 320, Loss: 5.4597\n",
      "Step 330, Loss: 5.5585\n",
      "Step 340, Loss: 5.8804\n",
      "Step 350, Loss: 6.4820\n",
      "Step 360, Loss: 6.3480\n",
      "Step 370, Loss: 6.2607\n",
      "Step 380, Loss: 5.8485\n",
      "Step 390, Loss: 5.3558\n",
      "Step 400, Loss: 5.5866\n",
      "Step 410, Loss: 4.9188\n",
      "Step 420, Loss: 6.1298\n",
      "Step 430, Loss: 6.1661\n",
      "Step 440, Loss: 5.8436\n",
      "Step 450, Loss: 3.8773\n",
      "Step 460, Loss: 4.6223\n",
      "Step 470, Loss: 5.6534\n",
      "Step 480, Loss: 5.8994\n",
      "Step 490, Loss: 5.9027\n",
      "Step 500, Loss: 6.2385\n",
      "Step 510, Loss: 7.1850\n",
      "Step 520, Loss: 5.8090\n",
      "Validation Loss: 7.4115\n",
      "Checkpoint saved at baby_joey_checkpoint_epoch_3.pt\n"
     ]
    }
   ],
   "source": [
    "from src.data.data import BabyJoeyDataset, BabyJoeyDataLoader\n",
    "from src.model.model import BabyJoeyModel\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "import torch.nn.functional as F\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    vocab_size: int\n",
    "    n_embd: int\n",
    "    n_head: int\n",
    "    n_layers: int\n",
    "    max_seq_len: int\n",
    "    padding_idx: int  # Index of the padding token\n",
    "    dropout_rate: float = 0.1  # Default dropout rate\n",
    "\n",
    "# Sample configuration\n",
    "config = ModelConfig(\n",
    "    vocab_size=50257,  # Example vocabulary size\n",
    "    n_embd=768,\n",
    "    n_head=12,\n",
    "    n_layers=12,\n",
    "    max_seq_len=1024,\n",
    "    padding_idx=50256,  # Padding token index\n",
    "    dropout_rate=0.1\n",
    ")\n",
    "\n",
    "dataset_instance = BabyJoeyDataset()\n",
    "training_dataset, validation_dataset = dataset_instance.load_or_create_datasets()\n",
    "\n",
    "data_loader_instance = BabyJoeyDataLoader(training_dataset, validation_dataset)\n",
    "train_loader, val_loader = data_loader_instance.get_dataloaders()\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BabyJoeyModel(config).to(device)\n",
    "model.train()\n",
    "\n",
    "# Use AdamW optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=3e-4)  # Example learning rate\n",
    "\n",
    "# Simple training loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        input_ids = batch['input_ids'].to(device)  # [batch_size, seq_len]\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(input_ids)  # [batch_size, seq_len, vocab_size]\n",
    "\n",
    "        # Shift inputs and logits for causal language modeling\n",
    "        shifted_logits = logits[:, :-1, :].contiguous()   # [batch_size, seq_len-1, vocab_size]\n",
    "        shifted_input_ids = input_ids[:, 1:].contiguous()  # [batch_size, seq_len-1]\n",
    "\n",
    "        # Compute loss with label smoothing and ignoring padding tokens\n",
    "        loss = F.cross_entropy(\n",
    "            shifted_logits.view(-1, config.vocab_size),\n",
    "            shifted_input_ids.view(-1),\n",
    "            ignore_index=config.padding_idx,\n",
    "            label_smoothing=0.1\n",
    "        )\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (step + 1) % 10 == 0:\n",
    "            print(f\"Step {step+1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_steps = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            logits = model(input_ids)\n",
    "\n",
    "            shifted_logits = logits[:, :-1, :].contiguous()\n",
    "            shifted_input_ids = input_ids[:, 1:].contiguous()\n",
    "\n",
    "            val_batch_loss = F.cross_entropy(\n",
    "                shifted_logits.view(-1, config.vocab_size),\n",
    "                shifted_input_ids.view(-1),\n",
    "                ignore_index=config.padding_idx,\n",
    "                label_smoothing=0.1\n",
    "            )\n",
    "\n",
    "            val_loss += val_batch_loss.item()\n",
    "            val_steps += 1\n",
    "\n",
    "    avg_val_loss = val_loss / max(val_steps, 1)\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "    model.train()\n",
    "\n",
    "    # Save a checkpoint after each epoch\n",
    "    checkpoint_path = f\"baby_joey_checkpoint_epoch_{epoch+1}.pt\"\n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'val_loss': avg_val_loss,\n",
    "        'config': config,\n",
    "    }, checkpoint_path)\n",
    "    print(f\"Checkpoint saved at {checkpoint_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.data import BabyJoeyDataset, BabyJoeyDataLoader\n",
    "from src.model.model import BabyJoeyModel\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    vocab_size: int\n",
    "    n_embd: int\n",
    "    n_head: int\n",
    "    n_layers: int\n",
    "    max_seq_len: int\n",
    "    padding_idx: int  # Index of the padding token\n",
    "    dropout_rate: float = 0.1  # Default dropout rate\n",
    "\n",
    "# Sample configuration\n",
    "config = ModelConfig(\n",
    "    vocab_size=50257,  # Example vocabulary size\n",
    "    n_embd=512,\n",
    "    n_head=8,\n",
    "    n_layers=1,\n",
    "    max_seq_len=512,\n",
    "    padding_idx=50256,  # Padding token index\n",
    "    dropout_rate=0.1\n",
    ")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_instance = BabyJoeyDataset()\n",
    "training_dataset, validation_dataset = dataset_instance.load_or_create_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_instance = BabyJoeyDataLoader(training_dataset, validation_dataset)\n",
    "train_loader, val_loader = data_loader_instance.get_dataloaders()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = samples['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_text = dataset_instance.tokenizer.decode(sample, skip_special_tokens=True)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BabyJoeyModel(config).to(device)\n",
    "# model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up optimizer\n",
    "optimizer = Adam(model.parameters(), lr=3e-4)  # Example learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Simple training loop\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        input_ids = batch['input_ids'].to(device)  # [batch_size, seq_len]\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(input_ids)  # [batch_size, seq_len, vocab_size]\n",
    "\n",
    "        # Shift inputs and logits for causal language modeling\n",
    "        # Targets: predict input_ids[t] from input_ids[:t]\n",
    "        # We drop the last token from logits and the first token from targets\n",
    "        shifted_logits = logits[:, :-1, :].contiguous()   # [batch_size, seq_len-1, vocab_size]\n",
    "        shifted_input_ids = input_ids[:, 1:].contiguous()  # [batch_size, seq_len-1]\n",
    "\n",
    "        # Flatten for cross-entropy\n",
    "        loss = F.cross_entropy(\n",
    "            shifted_logits.view(-1, config.vocab_size),\n",
    "            shifted_input_ids.view(-1)\n",
    "        )\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (step + 1) % 10 == 0:\n",
    "            print(f\"Step {step+1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Validation loop (optional)\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_steps = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            logits = model(input_ids)\n",
    "\n",
    "            shifted_logits = logits[:, :-1, :].contiguous()\n",
    "            shifted_input_ids = input_ids[:, 1:].contiguous()\n",
    "\n",
    "            loss = F.cross_entropy(\n",
    "                shifted_logits.view(-1, config.vocab_size),\n",
    "                shifted_input_ids.view(-1)\n",
    "            )\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            val_steps += 1\n",
    "    avg_val_loss = val_loss / max(val_steps, 1)\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
